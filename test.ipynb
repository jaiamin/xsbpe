{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from xsbpe.basic import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Jessica, looking down the length of the table, saw a faint trembling at the corners of Leto's mouth, noted the dark flush of anger on his cheeks. What has angered him? she asked herself. Surely not my invitation to the smuggler.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 101, 115, 115, 105, 99, 97, 44, 32, 108, 111, 111, 107, 105, 110, 103, 32, 100, 111, 119, 110, 32, 116, 104, 101, 32, 108, 101, 110, 103, 116, 104, 32, 111, 102, 32, 116, 104, 101, 32, 116, 97, 98, 108, 101, 44, 32, 115, 97, 119, 32, 97, 32, 102, 97, 105, 110, 116, 32, 116, 114, 101, 109, 98, 108, 105, 110, 103, 32, 97, 116, 32, 116, 104, 101, 32, 99, 111, 114, 110, 101, 114, 115, 32, 111, 102, 32, 76, 101, 116, 111, 39, 115, 32, 109, 111, 117, 116, 104, 44, 32, 110, 111, 116, 101, 100, 32, 116, 104, 101, 32, 100, 97, 114, 107, 32, 102, 108, 117, 115, 104, 32, 111, 102, 32, 97, 110, 103, 101, 114, 32, 111, 110, 32, 104, 105, 115, 32, 99, 104, 101, 101, 107, 115, 46, 32, 87, 104, 97, 116, 32, 104, 97, 115, 32, 97, 110, 103, 101, 114, 101, 100, 32, 104, 105, 109, 63, 32, 115, 104, 101, 32, 97, 115, 107, 101, 100, 32, 104, 101, 114, 115, 101, 108, 102, 46, 32, 83, 117, 114, 101, 108, 121, 32, 110, 111, 116, 32, 109, 121, 32, 105, 110, 118, 105, 116, 97, 116, 105, 111, 110, 32, 116, 111, 32, 116, 104, 101, 32, 115, 109, 117, 103, 103, 108, 101, 114, 46]\n",
      "LEN: 228\n"
     ]
    }
   ],
   "source": [
    "tk = BasicTokenizer()\n",
    "# only using individual chars, no merging, long\n",
    "encoded1 = tk.encode(sentence)\n",
    "print(encoded1)\n",
    "print('LEN:', len(encoded1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length: 203588 | Time to train: 4.953943729400635\n",
      "[74, 284, 115, 105, 99, 97, 268, 108, 111, 111, 107, 279, 32, 100, 295, 110, 275, 108, 269, 103, 257, 32, 281, 275, 116, 97, 98, 108, 101, 268, 115, 97, 119, 286, 32, 102, 97, 262, 261, 116, 280, 109, 98, 108, 279, 286, 261, 264, 99, 282, 110, 263, 260, 281, 32, 76, 101, 277, 39, 260, 109, 267, 257, 268, 110, 111, 116, 271, 264, 100, 274, 107, 32, 102, 108, 117, 115, 104, 32, 281, 32, 266, 103, 263, 32, 270, 283, 285, 99, 104, 101, 101, 107, 115, 265, 87, 104, 290, 104, 97, 260, 266, 103, 263, 271, 104, 105, 109, 63, 276, 288, 97, 115, 107, 271, 104, 263, 115, 101, 108, 102, 265, 83, 117, 280, 108, 278, 110, 111, 261, 109, 278, 262, 118, 105, 116, 97, 289, 270, 32, 277, 275, 115, 109, 117, 103, 103, 108, 263, 46]\n",
      "LEN: 150\n"
     ]
    }
   ],
   "source": [
    "text = open('dune.txt').read()\n",
    "\n",
    "st = time.time()\n",
    "tk.train(text, 300, verbose=False)\n",
    "et = time.time()\n",
    "\n",
    "print(f'Text Length: {len(text.split())} | Time to train: {et-st}')\n",
    "encoded2 = tk.encode(sentence)\n",
    "print(encoded2)\n",
    "print('LEN:', len(encoded2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length: 203588 | Time to train: 55.12827491760254\n",
      "[431, 268, 537, 279, 322, 433, 275, 108, 269, 103, 257, 754, 116, 403, 330, 415, 365, 286, 311, 366, 261, 116, 402, 533, 593, 528, 99, 914, 900, 351, 908, 321, 109, 953, 268, 331, 416, 264, 100, 461, 311, 108, 329, 104, 681, 493, 328, 270, 369, 342, 925, 107, 784, 777, 898, 493, 263, 852, 63, 543, 780, 857, 491, 947, 699, 417, 420, 584, 262, 118, 301, 469, 979, 115, 109, 117, 103, 785, 263, 46]\n",
      "LEN: 80\n"
     ]
    }
   ],
   "source": [
    "tk2 = BasicTokenizer()\n",
    "st = time.time()\n",
    "tk2.train(text, 1000, verbose=False)t\n",
    "et = time.time()\n",
    "\n",
    "print(f'Text Length: {len(text.split())} | Time to train: {et-st}')\n",
    "encoded2 = tk2.encode(sentence)\n",
    "print(encoded2)\n",
    "print('LEN:', len(encoded2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
